# A100 GPU Optimized Configuration for Final Training

# Data Configuration
data:
  target_stocks: ['NVDA', 'MSFT', 'AAPL', 'PLTR', 'CRM', 'ADBE', 'INTC', 'AMD']
  context_length: 30  # Increased for better temporal understanding
  prediction_horizon: 5

# Debug Model Configuration  
model:
  # Scaled up for A100 GPU
  hidden_layer_size: 128  # Increased from 64
  attention_head_size: 8  # Increased from 4
  dropout_rate: 0.1
  max_gradient_norm: 0.01
  learning_rate: 0.0005  # Stable learning rate
  num_encoder_steps: 30  # Match context_length
  num_steps: 5  # Match prediction_horizon

# Debug Training Configuration
training:
  # Optimized for A100
  num_epochs: 100  # Full training run
  minibatch_size: 128  # Increased batch size for A100
  early_stopping_patience: 15
  use_model_checkpoint: true
  use_reduce_lr_on_plateau: true
  lr_schedule_factor: 0.7
  lr_schedule_patience: 8
  min_lr: 0.00001
  
# Output Configuration
output:
  model_dir: '/content/drive/MyDrive/colab_training/outputs/tft_model_final'
  save_training_plots: true 